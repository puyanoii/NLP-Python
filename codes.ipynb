{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e239978c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Natural Language Processing in Python Lecture\n",
    "\n",
    "June 2021\n",
    "\n",
    "Pu Yan, Oxford Internet Institute, University of Oxford \n",
    "\n",
    "Email: <pu.yan@oii.ox.ac.uk> or <thuyanpu@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dfe4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Outline of the course\n",
    "\n",
    "0. Introduction\n",
    "    \n",
    "1. Text cleaning and pre-processing\n",
    "\n",
    "    1.1 Importing text data & summary of text \n",
    "    \n",
    "    1.2 Removal of punctuations\n",
    "    \n",
    "    1.3 Tokenisation\n",
    "    \n",
    "    1.4 Case normalisation\n",
    "    \n",
    "    1.5 Stopwords removal\n",
    "    \n",
    "    1.6 Stemming and Lemmatisation\n",
    "    \n",
    "    1.7 Advanced: Part-of-Speech tagging; named entities in spaCy\n",
    "\n",
    "2. Text analysis and visualisation\n",
    "\n",
    "    2.1 Word counts (pre- and post-processing)\n",
    "    \n",
    "    2.2 Word frequency\n",
    "    \n",
    "    2.3 TF-IDF: table and visualisation\n",
    "    \n",
    "3. Beyond quantification? Toolkit for qualitative research:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9657d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The importance of text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979fc87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Garbage in, Garbage out!](img/cleaning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdbbe56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Useful resources\n",
    "\n",
    "- Books:\n",
    "    \n",
    "    - Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied text analysis with Python : Enabling language-aware data products with machine learning. Beijing: O'Reilly\n",
    "    \n",
    "    - Vanderplas, J. (2017). Python data science handbook : Essential tools for working with data. Beijing: O'Reilly\n",
    "    \n",
    "- Online resources:\n",
    "    \n",
    "    - A list of NLP courses around the world, curated by ACL (the Association for Computational Linguistics): https://aclweb.org/aclwiki/List_of_NLP/CL_courses \n",
    "    \n",
    "    - An introduction to Natural Language Processing (NLP): https://port.sas.ac.uk/mod/book/view.php?id=583&chapterid=445 \n",
    "    \n",
    "    - Introduction to NLP & Data Science: https://www.youtube.com/watch?v=5BVebXXb2o4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3374de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 0. Introduction\n",
    "\n",
    "- Natural language processing (NLP) is becoming increasingly popular in social science\n",
    "\n",
    "    - Digitalisation of content data: i.e. Google Books, Newspaper digital database (LexisNexis)\n",
    "    \n",
    "    - The prevalence of social media platforms: i.e. Twitter, YouTube\n",
    "    \n",
    "    - Large-scale data: A paradigm shift in social science research?\n",
    "    \n",
    "- Advanages and disadvantages of NLP compared to qualitative methods\n",
    "\n",
    "    - Pros: Speed of analysis; Affordability (translation/transcription)\n",
    "    \n",
    "    - Cons: Accuracy (identifying sarcasm, jokes or irony); unstructured (compared to survey data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7c49c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 0.1 Implications of NLP in social science and humanities\n",
    "\n",
    "- Machine translation: \"你好\"<->“Hi”\n",
    "\n",
    "- Speech recognition: \"Hi, Siri\"\n",
    "\n",
    "- Sentiment analysis: understanding product reviews\n",
    "\n",
    "- Information extraction: automatically generated abstract/keywords list\n",
    "\n",
    "- Document classification: LDA topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceedafb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 0.2 Packages\n",
    "\n",
    "Please ensure that you have installed the following libraries in your local enviornment: \n",
    "    \n",
    "    - pandas (for loading)\n",
    "\n",
    "    - NLTK\n",
    "    \n",
    "    - spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bc403",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installing packages required for this course \n",
    "## !! DO NOT RUN this cell if you have already installed the three packages \n",
    "## Use the following code to install three packages using pip (if you have already installed pip). \n",
    "## Or, you can choose to install the packages in Anaconda Navigator: https://docs.anaconda.com/anaconda/navigator/tutorials/pandas/ \n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "\n",
    "%pip install --user -U nltk\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "%pip install --upgrade gensim\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "%pip install pillow\n",
    "%pip install wordcloud\n",
    "\n",
    "## Run the following code if you see warning message when importing gensim\n",
    "#%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788c120",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We will need to import the packages before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c46a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading packages required for this course\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "## download wordnet, stopwords  if you have not downloaded nltk's wordnet and stopwords data\n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords') \n",
    "\n",
    "#nltk.download() # You can use this line to start the NLTK Downloader and download all the data you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9ac32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 0.3 Dataset\n",
    "\n",
    "Now, we will load the dataset (thanks to Justin Ho, who prepared the dataset!) from the \"./data\" folder.\n",
    "\n",
    "The dataset includes social media posts from four parties during the 2021 Scottish Parliament election (Date range: 2021-02-06 ~ 2021-05-06):\n",
    "\n",
    "Scottish National Party, Scottish Conservatives, Scottish Labour, Scottish Greens, Scottish Liberal Democrats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634de4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"data/scotelection2021.csv\")\n",
    "\n",
    "# There are 8 columns in the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af9feb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's have a glimpse of the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047bd77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Often in crawlled dataset, we might find some rows do not contain the text data to analyse. We will make sure these rows are removed before the text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb88c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking how many rows miss \"text\" information\n",
    "len(df[df.text.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ede3a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing rows where \"text\" column is NA from the dataframe\n",
    "df = df[-df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36aacb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Which party gets the highest number of likes on average? Which party shares the highest number of posts on Facebook? \n",
    "\n",
    "Before analysing the text data, let's see the average likes each party received and total number of posts each party shared on Facebook during the data collection period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfd179",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouping by party names and calculating the average number of likes and total number of social media posts for each party.\n",
    "df.groupby(['snsname']).agg({'likes': 'mean', \n",
    "                             'text': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3968ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It seems that **SNP** receives the highest number of likes on average, but **Scottish Green Party** is the most active party on Facebook, calculated by the total number of posts each party shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a68d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We therefore will focus on posts from SNP for the linguistic analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a6d94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a subset of text dataset for all SNP Facebook posts \n",
    "df_snp = df[(df[\"snsname\"] == \"Scottish National Party (SNP)\")]\n",
    "df_snp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699bc47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Text cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c544e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.1 Importing text data & summary of text \n",
    "\n",
    "We will start with creating a summary statistics of the text data: \n",
    "\n",
    "- How many social media posts do we have in the dataset? \n",
    "- What is the length of each post? \n",
    "- What is the average lenghth of the Facebook posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a650428",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We dropped the following columns from the snp dataset: snshandle, comments, shares, postlink\n",
    "df_snp = df_snp.drop(columns=['snshandle', 'comments', 'shares', 'postlink'])\n",
    "\n",
    "# We calculate the number of characters we need to process, we create a new column \"word_count\" to calculate the number of characters in each post\n",
    "# Basically, we count the spaces but add one to calculate the number of words\n",
    "df_snp['word_count'] = df['text'].str.count(' ') + 1\n",
    "\n",
    "# Here's the structure of the final dataframe\n",
    "df_snp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d363c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pandas display setting:\n",
    "# We want to see longer sentences in the text column in jupyter notebook\n",
    "# We reset the \"max_colwidth\" in the pandas display options to display all characters in the text column.\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Let's see the most popular five media posts by snp (measutred by Facebook likes) \n",
    "df_snp.sort_values(by='likes', ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cfcf7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the average number of words in the social media posts by SNP\n",
    "print(\"There are {} social media posts from SNP in the 2021 election dataset\\n\".format(df_snp.word_count.count()))\n",
    "print(\"The average number of words in SNP social media posts during 2021 election is: {}\\n\".format(df_snp.word_count.mean()))\n",
    "print(\"Total number of words in SNP social media posts during 2021 election is: {}\\n\".format(df_snp.word_count.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf17b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's visualise the histogram of word counts and likes in SNP social media posts\n",
    "df_snp.hist(column='word_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe89cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Removal of punctuations & 1.3 Tokenisation & 1.4 Case normalisation\n",
    "\n",
    "In text cleaning process, we also want to remove punctuations, non-text symbols, or hyper-links in the text.\n",
    "\n",
    "We also need to **tokenise** sentences into separated words and normalise capitalised words\n",
    "\n",
    "By using gensim's simple_preprocess function, we can finish the three steps using one line of code! (THANK YOU GENSIM!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423fabc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Transforming sentences into tokens: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b218965",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![tokenising sentences](img/tokenising.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effee8df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before preprocessing, we first convert the text column into a list\n",
    "data = df_snp.text.values.tolist()\n",
    "\n",
    "# Here's how the raw text data looks like\n",
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4c47d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removal of hyper-link\n",
    "import re\n",
    "def link_removal(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(re.sub(r'http\\S+', '', sentence))\n",
    "\n",
    "data_nolink = list(link_removal(data))\n",
    "data_nolink[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9ff34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# We write a function to 1) tokenise sentences into words, and also 2) remove punctuations in the sentences \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Now, let's apply the tokenisation and removel of punctuations to all social media posts\n",
    "# You will find all punctuations (including emojis, hashtag symbols) are removed from the sentence.\n",
    "data_words = list(sent_to_words(data_nolink))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c737de1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let see our pre-processing progress so far\n",
    "example_df_1 = pd.DataFrame(\n",
    "    {'raw text': data,\n",
    "     'removal of punctuations, tokenised, normalised': data_words\n",
    "    })\n",
    "example_df_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30763b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.5 Stop words removal\n",
    "\n",
    "- **Stop words** are high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. (See details in this link: https://www.nltk.org/book/ch02.html)\n",
    "\n",
    "- The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37848ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing NLTK stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# You can append the list of stop_words using the code below,\n",
    "# simple replace \"yout_stop_words\" with the word(s) you want to filter out from the corpus\n",
    "stop_words.extend(['www', 'https'])\n",
    "\n",
    "# We define a function to filter out words that appear in the stopwords list from NLTK English stopwords library\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "# We now apply the stopwords filtering functions on tokenised&normalised text\n",
    "data_words_nostopwords = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2fd68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see which words are included in the NLTK stopwords list\n",
    "print(stop_words, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d3464",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let see our pre-processing progress so far\n",
    "example_df_2 = pd.DataFrame(\n",
    "    {'raw text': data,\n",
    "     'removal of punctuations, tokenised, normalised': data_words,\n",
    "     'stop words removed': data_words_nostopwords\n",
    "    })\n",
    "example_df_2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdd62a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.6 Stemming and Lemmatisation\n",
    "\n",
    "The goal of both **stemming and lemmatization** is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "    i.e am, are, is --> be \n",
    "\n",
    "**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "    i.e. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c0f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# We define a function to lookup lemmas of each word in the post\n",
    "def lemmatise(texts):\n",
    "    return [[wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in doc] for doc in texts]\n",
    "\n",
    "data_lemmatise = lemmatise(data_words_nostopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b261b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "*When to use what?*\n",
    "\n",
    "- Stemming and Lemmatization both generate the root form of the inflected words; \n",
    "\n",
    "- The difference is that stem might not be an actual word whereas, lemma is an actual language word;\n",
    "\n",
    "- In lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f39a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let see our pre-processing progress so far\n",
    "example_df_3 = pd.DataFrame(\n",
    "    {'raw text': data,\n",
    "     'removal of punctuations, tokenised, normalised': data_words,\n",
    "     'stop words removed': data_words_nostopwords,\n",
    "     'lemmatised words': data_lemmatise\n",
    "    })\n",
    "example_df_3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611da96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.7 Advanced: Part-of-Speech tagging; named entities in spaCy\n",
    "#### PoS tagging\n",
    "In NLP pre-processing, we often need to focus on \"meaningful\" words such as nouns, verbs, and adjectives. \n",
    "\n",
    "spaCy use *trained pipilines* and *statistical models* to predict Part-of-Speech tags in the document. \n",
    "\n",
    "This is a system that make *predictions* on the Part of Speech of each word in the sentence, for example, a word following “the” in English is most likely a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5958",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tag \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# fine-grained and coarse-grained part-of-speech tags in non-English languages: https://spacy.io/models\n",
    "## POS tag scheme for English\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Let's visualise the Part-of-Speech tags\n",
    "pos_example = nlp(u'Can digital computers think? by Alan Turing, a computer scientist born in London')\n",
    "displacy.render(pos_example, style = 'dep', jupyter = True, options = {'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891fb77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Named entities\n",
    "- A named entity is a “real-world object” that’s assigned a name – for example, a person (Alan Turing), a city (London), a product or a book title. \n",
    "- We can use spaCy to recognize various types of named entities in a document\n",
    "- Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some fine-tuning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea3580",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Named entities recognition\n",
    "displacy.render(pos_example, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e0a51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How to include meaningful words ONLY in the corpus by using the POS tagging?\n",
    "import spacy\n",
    "\n",
    "# We need to use spacy for pos-tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a pos-tagging processor, allowing only nounds, adjectives, verbs, and adverbs in the corpus\n",
    "def postags_filter(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Now, keeping only noun, adj, vb, adv\n",
    "data_postag = postags_filter(data_lemmatise, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa302f0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let see our pre-processing progress so far\n",
    "example_df_4 = pd.DataFrame(\n",
    "    {'raw text': data,\n",
    "     'removal of punctuations, tokenised, normalised': data_words,\n",
    "     'stop words removed': data_words_nostopwords,\n",
    "     'lemmatised words': data_lemmatise,\n",
    "     'pos tagged': data_postag\n",
    "    })\n",
    "example_df_4.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9adad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. Text analysis and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db2e89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Word counts (pre- and post-processing)\n",
    "\n",
    "Let's now merge the processed text into the dataframe and generate a column for word counts after processing(text cleaning). We can see how much textual information has been filtered out from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d02431",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Corpus for TD-IDF analysis \n",
    "df_snp[\"text_cleaned\"] = data_postag # Use the data_lemmatized for text analysis\n",
    "df_snp[\"word_count_cleaned\"] = df_snp['text_cleaned'].str.len()\n",
    "df_snp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac89169",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Word frequency\n",
    "\n",
    "- An important question when analysing text data is: How to measure the key information in the corpus? \n",
    "\n",
    "- The most strait-forward way is to calculate **Term Frequency (TF)**, which is how frequency each word appear in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75c942",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We used NLTK's FreqDist to find the most common words in the corpora\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# We merged all pre-processed social media content from SNP into one corpora\n",
    "corpora = df_snp['text_cleaned'].sum()\n",
    "\n",
    "# And displayed the top 20 most common terms in the corpora\n",
    "fdist = FreqDist(corpora)\n",
    "top_20 = fdist.most_common(20)\n",
    "top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ab1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.3 TF-IDF: table and visualisation\n",
    "\n",
    "- **TF-IDF** With tf-idf, instead of representing a term in a document by its raw frequency (number of occurrences) or its relative frequency (term count divided by document length), each term is weighted by dividing the term frequency by the number of documents in the corpus containing the word\n",
    "\n",
    "- The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in a document are often the most frequently used words in all of the documents: i.e. the, a, not\n",
    "\n",
    "- Terms with the highest tf-idf scores are the terms in a document that are distinctively frequent in a document, when that document is compared other documents.\n",
    "\n",
    "More details on TF-IDF can be found here: https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d495d35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text processing steps like tokenization and removing punctuation will happen automatically when we use Scikit-Learn’s TfidfVectorizer \n",
    "# We therefore create a list of strings of the cleaned social media posts\n",
    "corpora_strings = [\" \".join(text) for text in df_snp['text_cleaned'].values]\n",
    "\n",
    "#import the TfidfVectorizer from Scikit-Learn.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Our corpora already removes common english stopwords, so we set stop_words = none\n",
    "vectorizer = TfidfVectorizer(stop_words=None, use_idf=True, norm=None) \n",
    "\n",
    "#The fit_transform() method converts the list of strings to something called a sparse matrix. \n",
    "#In this case, the matrix represents tf-idf values for all texts. \n",
    "transformed_documents = vectorizer.fit_transform(corpora_strings).todense()\n",
    "\n",
    "# We want to creat a matrix of word-document, where each document has the same number of values, one for each word in the corpus\n",
    "matrix_df = pd.DataFrame(transformed_documents, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Let's now get the tf-idf values for all words\n",
    "words_tfidf = matrix_df.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c584ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_tfidf[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0eebb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.4 Visualisation of keywords using tf-idf\n",
    "\n",
    "In the preliminary analysis of text data, we often want to show the results through visualisation. We can show the top ranked keywords using tf-idf scores and generate word clouds of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2faa46a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "## Creating a wordcloud using top ranked 100 words (measured by TD-IDF) for the SNP corpora\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "wc = WordCloud(background_color = 'white',\n",
    "              width=800,height=600,\n",
    "              max_words=2000).fit_words(words_tfidf[:100])\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675d95f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Want a more taylored visualisation for SNP? Sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bbb7f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "## Creating a masked wordcloud using SNP's logo\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "custom_mask = np.array(Image.open(\"img/snp.png\"))\n",
    "wc_2 = WordCloud(background_color = 'white',\n",
    "              width=800,height=600,\n",
    "              mask = custom_mask,\n",
    "              mode='RGBA',\n",
    "              max_words=2000).fit_words(words_tfidf[:100])\n",
    "image_colors = ImageColorGenerator(custom_mask)\n",
    "wc_2.recolor(color_func = image_colors)\n",
    "plt.imshow(wc_2, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b2b28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Beyond quantification? Toolkit for qualitative research:\n",
    "\n",
    "- There are many ways to examine the **context** of a text apart from simply reading it. A concordance view shows us every occurrence of a given word, together with some context. \n",
    "\n",
    "- Here we look up the word *covid* in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c244098",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "corpora = df_snp['text_cleaned'].sum()\n",
    "df_snp['text'].values.tolist()\n",
    "textList = Text(corpora)\n",
    "textList.concordance('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d0310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Finally, let's do something creative, by generating a social media post that is in similar style as what we have seen in the SNP corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e640ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "textList.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69c536",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![ending](img/end.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
